# -*- coding: utf-8 -*-
"""Bitcoin Heist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u1fQsMeNlBqjlw1gLBK_t2EfS9FR4KIx

## Detection of Ransomware using Deep Learning
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import keras
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
sns.set_style("darkgrid")
# %matplotlib inline
import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/BitcoinHeistData.csv')

df.head(10)

# get descriptive statistics
df.describe()

# get the shape of the dataframe
print(df.shape)

# check for missing values
print(df.isnull().sum())

# drop date, year and address columns
df = df.drop(['year'], axis=1)
df = df.drop(['day'], axis=1)
df = df.drop(['address'], axis=1)

"""# Exploratory Data Analysis"""

# 1. Correlation matrix
corr = df.corr()
print(corr)

# 2. Heatmap
sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns)
plt.show()

# #3. Distribution of each feature
# sns.distplot(df['income'])
# plt.savefig("income-skewed")

# sns.distplot(df['weight'])
# plt.savefig("weight-skewed")

# sns.distplot(df['count'])
# plt.savefig("count-skewed")

# sns.distplot(df['looped'])
# plt.savefig("looped-skewed")

# sns.distplot(df['neighbors'])
# plt.savefig("neighbours-skewed")

# sns.distplot(df['length'])
# plt.savefig("length-skewed")

# #5. Boxplot
# sns.boxplot(df['income'])
# plt.show()

# sns.boxplot(df['weight'])
# plt.show()

# sns.boxplot(df['count'])
# plt.show()

# sns.boxplot(df['looped'])
# plt.show()

# sns.boxplot(df['neighbors'])
# plt.show()

# sns.boxplot(df['length'])
# plt.show()

print(df['label'].value_counts())

# show the % of each label
print(df['label'].value_counts(normalize=True)*100)

# show the barchart of white label vs rest of the labels treated as black
label_white = []
label_black = []
for element in df.label:
    if element == 'white':
        label_white.append(element)
    else:
        label_black.append(element)

x_ax = ['White','Black']
y_ax = [len(label_white),len(label_black)]

plt.figure(figsize = (8,6))
plt.bar(x_ax,y_ax)
plt.savefig("imbalaced")

# make right skewed data to normal distribution
df['income'] = np.log(df['income'])
df['weight'] = np.log(df['weight'])
df['count'] = np.log(df['count'])
df['looped'] = np.log(df['looped'])
df['neighbors'] = np.log(df['neighbors'])
df['length'] = np.log(df['length'])

df.shape

# drop nan values
df = df.dropna()

df.shape

# drop inf rows
df = df[np.isfinite(df['income'])]

df = df[np.isfinite(df['weight'])]

df = df[np.isfinite(df['count'])]

df = df[np.isfinite(df['looped'])]

df.shape

df = df[np.isfinite(df['neighbors'])]

df.shape

df = df[np.isfinite(df['length'])]

df.shape

# # distribution of each feature
# sns.distplot(df['income'])
# plt.show()
# sns.distplot(df['weight'])
# plt.show()
# sns.distplot(df['count'])
# plt.show()
# sns.distplot(df['looped'])
# plt.show()
# sns.distplot(df['neighbors'])
# plt.show()
# sns.distplot(df['length'])
# plt.show()

# correlation matrix
# corr = df.corr()
# # print(corr)

# # heatmap
# sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns)
# plt.show()

# convert label to categorical
df['label'] = df['label'].astype('category')
# make a copy of the dataframe
df_copy = df.copy()
# perform get_dummies on label
label_encode = pd.get_dummies(df_copy.label)
label_encode

# make a new column called 'label_encoded' in df
df['label_encoded'] = label_encode.white
# drop label column
df = df.drop(['label'], axis=1)
# reset index
df = df.reset_index(drop=True)
# get top 5 rows of the dataframe
print(df.head())

df.shape

#corelation matrix
corr = df.corr()
# print(corr)

# heatmap
sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns)
plt.show()

from sklearn.ensemble import IsolationForest

check_cols=['length','weight','count','looped','neighbors','income']
train_set=df[check_cols]
train_set.head()

clf=IsolationForest(n_jobs=6,random_state=23,contamination=0.01)
clf.fit(train_set)
y_pred_train=clf.predict(train_set)

pd.value_counts(y_pred_train)

X_cleaned=train_set[y_pred_train==1]

y_cleaned=df['label_encoded'][y_pred_train==1]

X_cleaned

y_cleaned.value_counts()

# train_set.loc[y_pred_train==-1,:]

# abn_ind=np.where(y_pred_train<0)[0]
# abn_ind

# x=df.values

# plt.scatter(df.iloc[:,0],df.iloc[:,1])

# plt.scatter(x[abn_ind,0],x[abn_ind,4],edgecolors="r")

# # if neighbours is greater than log(2) then encode it as 0 else encode it as 1
# df['neighbors'] = np.where(df['neighbors'] > np.log(2), 0, 1)
# # if length is greater than log(8) then encode it as 0 else encode it as 1
# df['length'] = np.where(df['length'] > np.log(8), 0, 1)
# # if count is greater than log(1) then encode it as 0 else encode it as 1
# df['count'] = np.where(df['count'] > np.log(1), 0, 1)
# # if looped is greater than log(1) then encode it as 0 else encode it as 1
# df['looped'] = np.where(df['looped'] > np.log(1), 0, 1)

# if neighbours is greater than log(2) then encode it as 0 else encode it as 1
df2['neighbors'] = np.where(df2['neighbors'] > np.log(2), 0, 1)
# if length is greater than log(8) then encode it as 0 else encode it as 1
df2['length'] = np.where(df2['length'] > np.log(8), 0, 1)
# if count is greater than log(1) then encode it as 0 else encode it as 1
df2['count'] = np.where(df2['count'] > np.log(1), 0, 1)
# if looped is greater than log(1) then encode it as 0 else encode it as 1
df2['looped'] = np.where(df2['looped'] > np.log(1), 0, 1)

# # corelation matrix
# corr = df.corr()
# # print(corr)
# # heatmap
# sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns)
# plt.show()

# corelation matrix
corr = df2.corr()
print(corr)
# heatmap
sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns)
plt.show()

# scale the except the label encoded column
df_copy = df.copy()
# scaler = StandardScaler()
# drop label column
df_copy = df_copy.drop(['label_encoded'], axis=1)
# fit the scaler
# scaler.fit(df_copy)
# transform the data
# df_copy = scaler.transform(df_copy)
# add the label encoded column back
df_copy = pd.concat([pd.DataFrame(df_copy), df['label_encoded']], axis=1)
# reset index
df_copy = df_copy.reset_index(drop=True)
# rename the columns
df_copy.columns = ['length', 'weight', 'count', 'looped', 'neighbors', 'income', 'label_encoded']
# get top 5 rows of the dataframe
print(df_copy.head())

# since majority of labels are white we need to balance the dataset
# create a new dataframe with only white label
df_white = df_copy[df_copy['label_encoded'] == 1]
# create a new dataframe with only black label
df_black = df_copy[df_copy['label_encoded'] == 0]
# get the number of rows in each dataframe
white_count = len(df_white)
black_count = len(df_black)

print(white_count,black_count)

# get the number of rows to be removed
remove_count = white_count - black_count
# get the index of the rows to be removed
if(remove_count>0):
  remove_index = df_white.sample(remove_count).index
  # drop the rows
  df_white = df_white.drop(remove_index)
else:
  remove_index = df_black.sample(abs(remove_count)).index
  # drop the rows
  df_black =df_black.drop(remove_index)

# merge the two dataframes
df_balanced = pd.concat([df_white, df_black], axis=0)
# reset index
df_balanced = df_balanced.reset_index(drop=True)
# get top 5 rows of the dataframe
print(df_balanced.head())

# check if the dataframe is balanced
print(df_balanced['label_encoded'].value_counts(normalize=True)*100)

bw_white = []
bw_black = []
for element in df_balanced.label_encoded:
    if element == 1:
        bw_white.append(element)
    else:
        bw_black.append(element)

x_ax = ['White','Black']
y_ax = [len(bw_white),len(bw_black)]

plt.figure(figsize = (8,6))
plt.bar(x_ax,y_ax)
plt.savefig("balanced.png")

# corelation matrix
corr = df_balanced.corr()
print(corr)
# heatmap
sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns)

# split the dataframe into train and test
X = df_balanced.drop(['label_encoded'], axis=1)
y = df_balanced['label_encoded']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)


# scale the data
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
# fit the scaler
scaler.fit(X_train)
# transform the data
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

# scale the data by MinMaxScaler
from sklearn.preprocessing import MinMaxScaler
scaler3 = MinMaxScaler()
# fit the scaler
scaler3.fit(X_train)
# transform the data
X_train = scaler3.transform(X_train)
X_test = scaler3.transform(X_test)

# scale the data by RobustScaler
from sklearn.preprocessing import RobustScaler
# scale the data
scaler2 = RobustScaler()
# fit the scaler
scaler2.fit(X_train)
# transform the data
X_train = scaler2.transform(X_train)
X_test = scaler2.transform(X_test)

# Logistic Regression model
from sklearn.linear_model import LogisticRegression
# create a logistic regression model
log_reg = LogisticRegression()
# fit the model
log_reg.fit(X_train, y_train)
# predict the labels
y_pred = log_reg.predict(X_test)
# get the accuracy score
print(log_reg.score(X_test, y_test))

# get the confusion matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)

# get the classification report
print(classification_report(y_test, y_pred))

from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score
print('Recall Score: {:.2f}'.format(recall_score(y_test, y_pred)))
print('Precision Score: {:.2f}'.format(precision_score(y_test, y_pred)))
print('F1 Score: {:.2f}'.format(f1_score(y_test, y_pred)))
print('Accuracy Score: {:.2f}'.format(accuracy_score(y_test, y_pred)))

# get the roc_auc_score
print(roc_auc_score(y_test, y_pred))

# get the roc curve
fpr1, tpr1, thresholds1 = roc_curve(y_test, y_pred)
# Plot ROC curve
plt.figure(figsize = (12,8))
roc1 = roc_auc_score(y_test, y_pred)
plt.plot(fpr1, tpr1, label='ROC curve (area = %0.3f)' % roc1)
plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate or (1 - Specifity)')
plt.ylabel('True Positive Rate or (Sensitivity)')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")

# KNN model
from sklearn.neighbors import KNeighborsClassifier
# create a KNN model
knn = KNeighborsClassifier(n_neighbors=5)
# fit the model
knn.fit(X_train, y_train)
# predict the labels
y_pred = knn.predict(X_test)
# get the accuracy score
print(knn.score(X_test, y_test))

# get the confusion matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)

# get the classification report
print(classification_report(y_test, y_pred))

from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score
print('Recall Score: {:.2f}'.format(recall_score(y_test, y_pred)))
print('Precision Score: {:.2f}'.format(precision_score(y_test, y_pred)))
print('F1 Score: {:.2f}'.format(f1_score(y_test, y_pred)))
print('Accuracy Score: {:.2f}'.format(accuracy_score(y_test, y_pred)))

# get the roc_auc_score
print(roc_auc_score(y_test, y_pred))

# get the roc curve
fpr2, tpr2, thresholds2 = roc_curve(y_test, y_pred)
# Plot ROC curve
plt.figure(figsize = (12,8))
roc2 = roc_auc_score(y_test, y_pred)
plt.plot(fpr2, tpr2, label='ROC curve (area = %0.3f)' % roc2)
plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate or (1 - Specifity)')
plt.ylabel('True Positive Rate or (Sensitivity)')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")

# support vector machine model
from sklearn.svm import SVC
# create a SVM model
svm = SVC(kernel='rbf', gamma=0.1, C=1)
# fit the model
svm.fit(X_train, y_train)
# predict the labels
y_pred = svm.predict(X_test)
# get the accuracy score
print(svm.score(X_test, y_test))

# get the confusion matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)

# get the classification report
print(classification_report(y_test, y_pred))

from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score
print('Recall Score: {:.2f}'.format(recall_score(y_test, y_pred)))
print('Precision Score: {:.2f}'.format(precision_score(y_test, y_pred)))
print('F1 Score: {:.2f}'.format(f1_score(y_test, y_pred)))
print('Accuracy Score: {:.2f}'.format(accuracy_score(y_test, y_pred)))

# get the roc_auc_score
print(roc_auc_score(y_test, y_pred))

# get the roc curve
fpr3, tpr3, thresholds3 = roc_curve(y_test, y_pred)
# Plot ROC curve
plt.figure(figsize = (12,8))
roc3 = roc_auc_score(y_test, y_pred)
plt.plot(fpr3, tpr3, label='ROC curve (area = %0.3f)' % roc3)
plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate or (1 - Specifity)')
plt.ylabel('True Positive Rate or (Sensitivity)')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")

# Decision Tree model
from sklearn.tree import DecisionTreeClassifier
# create a decision tree model
dec_tree = DecisionTreeClassifier()
# fit the model
dec_tree.fit(X_train, y_train)
# predict the labels
y_pred = dec_tree.predict(X_test)
# get the accuracy score
print(dec_tree.score(X_test, y_test))

# get the confusion matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)

# get the classification report
print(classification_report(y_test, y_pred))

from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score
print('Recall Score: {:.2f}'.format(recall_score(y_test, y_pred)))
print('Precision Score: {:.2f}'.format(precision_score(y_test, y_pred)))
print('F1 Score: {:.2f}'.format(f1_score(y_test, y_pred)))
print('Accuracy Score: {:.2f}'.format(accuracy_score(y_test, y_pred)))

# get the roc_auc_score
print(roc_auc_score(y_test, y_pred))

# get the roc curve
fpr4, tpr4, thresholds4 = roc_curve(y_test, y_pred)
# Plot ROC curve
plt.figure(figsize = (12,8))
roc4 = roc_auc_score(y_test, y_pred)
plt.plot(fpr4, tpr4, label='ROC curve (area = %0.3f)' % roc4)
plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate or (1 - Specifity)')
plt.ylabel('True Positive Rate or (Sensitivity)')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")

# Random Forest model
from sklearn.ensemble import RandomForestClassifier
# create a random forest model with 100 trees
rf = RandomForestClassifier(n_estimators=100)
# fit the model
rf.fit(X_train, y_train)
# predict the labels
y_pred = rf.predict(X_test)
# get the accuracy score
print(rf.score(X_test, y_test))

# get the confusion matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)

# get the classification report
print(classification_report(y_test, y_pred))

from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score
print('Recall Score: {:.2f}'.format(recall_score(y_test, y_pred)))
print('Precision Score: {:.2f}'.format(precision_score(y_test, y_pred)))
print('F1 Score: {:.2f}'.format(f1_score(y_test, y_pred)))
print('Accuracy Score: {:.2f}'.format(accuracy_score(y_test, y_pred)))

# get the roc_auc_score
print(roc_auc_score(y_test, y_pred))

# get the roc curve
fpr5, tpr5, thresholds5 = roc_curve(y_test, y_pred)
# Plot ROC curve
plt.figure(figsize = (12,8))
roc5 = roc_auc_score(y_test, y_pred)
plt.plot(fpr5, tpr5, label='ROC curve (area = %0.3f)' % roc5)
plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate or (1 - Specifity)')
plt.ylabel('True Positive Rate or (Sensitivity)')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")

# adaboost model
from sklearn.ensemble import AdaBoostClassifier
# create a adaboost model with 100 trees
ada = AdaBoostClassifier(n_estimators=100)
# fit the model
ada.fit(X_train, y_train)
# predict the labels
y_pred = ada.predict(X_test)
# get the accuracy score
print(ada.score(X_test, y_test))

# get the confusion matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)

# get the classification report
print(classification_report(y_test, y_pred))

from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score
print('Recall Score: {:.2f}'.format(recall_score(y_test, y_pred)))
print('Precision Score: {:.2f}'.format(precision_score(y_test, y_pred)))
print('F1 Score: {:.2f}'.format(f1_score(y_test, y_pred)))
print('Accuracy Score: {:.2f}'.format(accuracy_score(y_test, y_pred)))

# get the roc_auc_score
print(roc_auc_score(y_test, y_pred))

# get the roc curve
fpr6, tpr6, thresholds6 = roc_curve(y_test, y_pred)
# Plot ROC curve
plt.figure(figsize = (12,8))
roc6 = roc_auc_score(y_test, y_pred)
plt.plot(fpr6, tpr6, label='ROC curve (area = %0.3f)' % roc6)
plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate or (1 - Specifity)')
plt.ylabel('True Positive Rate or (Sensitivity)')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")

# gradient boosting model
from sklearn.ensemble import GradientBoostingClassifier
# create a gradient boosting model with 100 trees
gb = GradientBoostingClassifier(n_estimators=100)
# fit the model
gb.fit(X_train, y_train)
# predict the labels
y_pred = gb.predict(X_test)
# get the accuracy score
print(gb.score(X_test, y_test))

# get the confusion matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)

# get the classification report
print(classification_report(y_test, y_pred))

from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score
print('Recall Score: {:.2f}'.format(recall_score(y_test, y_pred)))
print('Precision Score: {:.2f}'.format(precision_score(y_test, y_pred)))
print('F1 Score: {:.2f}'.format(f1_score(y_test, y_pred)))
print('Accuracy Score: {:.2f}'.format(accuracy_score(y_test, y_pred)))

# get the roc_auc_score
print(roc_auc_score(y_test, y_pred))

# get the roc curve
fpr7, tpr7, thresholds7 = roc_curve(y_test, y_pred)
# Plot ROC curve
plt.figure(figsize = (12,8))
roc7 = roc_auc_score(y_test, y_pred)
plt.plot(fpr7, tpr7, label='ROC curve (area = %0.3f)' % roc7)
plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate or (1 - Specifity)')
plt.ylabel('True Positive Rate or (Sensitivity)')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")

# Artificial Neural Network model
from sklearn.neural_network import MLPClassifier
# create a neural network model
nn = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500)
# fit the model
nn.fit(X_train, y_train)
# predict the labels
y_pred = nn.predict(X_test)
# get the accuracy score
print(nn.score(X_test, y_test))

# get the confusion matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)

# get the classification report
print(classification_report(y_test, y_pred))

from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score
print('Recall Score: {:.2f}'.format(recall_score(y_test, y_pred)))
print('Precision Score: {:.2f}'.format(precision_score(y_test, y_pred)))
print('F1 Score: {:.2f}'.format(f1_score(y_test, y_pred)))
print('Accuracy Score: {:.2f}'.format(accuracy_score(y_test, y_pred)))

# get the roc_auc_score
print(roc_auc_score(y_test, y_pred))

# get the roc curve
fpr8, tpr8, thresholds8 = roc_curve(y_test, y_pred)
# Plot ROC curve
plt.figure(figsize = (12,8))
roc8 = roc_auc_score(y_test, y_pred)
plt.plot(fpr8, tpr8, label='ROC curve (area = %0.3f)' % roc8)
plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate or (1 - Specifity)')
plt.ylabel('True Positive Rate or (Sensitivity)')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")

# Artificial Neural Network model with multi-layer perceptron
from sklearn.neural_network import MLPClassifier
# create a multi-layer perceptron model
mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=1000)
# fit the model
mlp.fit(X_train, y_train)
# predict the labels
y_pred = mlp.predict(X_test)
# get the accuracy score
print(mlp.score(X_test, y_test))

# get the confusion matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)

# get the classification report
print(classification_report(y_test, y_pred))

from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score
print('Recall Score: {:.2f}'.format(recall_score(y_test, y_pred)))
print('Precision Score: {:.2f}'.format(precision_score(y_test, y_pred)))
print('F1 Score: {:.2f}'.format(f1_score(y_test, y_pred)))
print('Accuracy Score: {:.2f}'.format(accuracy_score(y_test, y_pred)))

# get the roc_auc_score
print(roc_auc_score(y_test, y_pred))

# get the roc curve
fpr9, tpr9, thresholds9 = roc_curve(y_test, y_pred)
# Plot ROC curve
plt.figure(figsize = (12,8))
roc9 = roc_auc_score(y_test, y_pred)
plt.plot(fpr9, tpr9, label='ROC curve (area = %0.3f)' % roc9)
plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate or (1 - Specifity)')
plt.ylabel('True Positive Rate or (Sensitivity)')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")

# get the classification report
print(classification_report(y_test, y_pred))

#keras Neural Network with 5 hidden layers with 'relu' activation function and adam optimizer
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
# from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
model = Sequential()
model.add(Dense(units=128, activation='relu', kernel_initializer='normal', input_dim=6))
model.add(Dropout(0.1))
model.add(Dense(units=128, activation='relu', kernel_initializer='normal'))
model.add(Dropout(0.1))
model.add(Dense(units=128, activation='relu', kernel_initializer='normal'))
model.add(Dropout(0.1))
model.add(Dense(units=128, activation='relu', kernel_initializer='normal'))
model.add(Dropout(0.1))
model.add(Dense(units=128, activation='relu', kernel_initializer='normal'))
model.add(Dropout(0.1))
model.add(Dense(units=1, activation='sigmoid'))
opt = tf.optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])
# fit the model
model.fit(X_train, y_train, epochs=150, batch_size=12)
# predict the labels
y_pred = model.predict(X_test)

# get the accuracy score
print(model.evaluate(X_test, y_test))

# convert y_pred to binary
y_pred = (y_pred > 0.5)

# get the confusion matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)

# get the classification report
print(classification_report(y_test, y_pred))

from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score
print('Recall Score: {:.2f}'.format(recall_score(y_test, y_pred)))
print('Precision Score: {:.2f}'.format(precision_score(y_test, y_pred)))
print('F1 Score: {:.2f}'.format(f1_score(y_test, y_pred)))
print('Accuracy Score: {:.2f}'.format(accuracy_score(y_test, y_pred)))

# get the roc_auc_score
print(roc_auc_score(y_test, y_pred))

# plot roc curve
# get the roc curve
fpr10, tpr10, thresholds10 = roc_curve(y_test, y_pred)
# Plot ROC curve
plt.figure(figsize = (12,8))
roc10 = roc_auc_score(y_test, y_pred)
plt.plot(fpr10, tpr10, label='ROC curve (area = %0.3f)' % roc10)
plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate or (1 - Specifity)')
plt.ylabel('True Positive Rate or (Sensitivity)')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")

X_train

# from sklearn.ensemble import IsolationForest
# # Create an Isolation Forest model
# random_state = np.random.RandomState(42)
# model=IsolationForest(n_estimators=100,max_samples='auto',contamination=float(0.5),random_state=random_state)


# # Fit the model (no need for y_train as this is unsupervised)
# model.fit(df_balanced[['label_encoded']])

# print(model.get_params())

# # # Predict anomalies (1 for inliers, -1 for outliers)
# # y_pred = model.predict(X_test)

# # # Convert the prediction results to 0 (inliers) and 1 (outliers)
# # y_pred_binary = [1 if pred == -1 else 0 for pred in y_pred]

# # # Print the accuracy score

# # print('Accuracy Score: {:.2f}'.format(accuracy_score(y_test, y_pred_binary)))

# df_balanced['scores'] = model.decision_function(df_balanced[['label_encoded']])

# df_balanced['anomaly_score'] = model.predict(df_balanced[['label_encoded']])

# df_balanced[df_balanced['anomaly_score']==-1].head()

# df_balanced.shape[0]

# accuracy = 100*list(df_balanced['anomaly_score']).count(-1)/df_balanced.shape[0]
# print("Accuracy of the model:", accuracy)

# weight
# count
# looped
# neighbors
# income
# label_encoded

# # Print the confusion matrix
# cm = confusion_matrix(y_test, y_pred_binary)
# print(cm)

# # Print the classification report
# print(classification_report(y_test, y_pred_binary))

# # Print additional metrics
# print('Recall Score: {:.2f}'.format(recall_score(y_test, y_pred_binary)))
# print('Precision Score: {:.2f}'.format(precision_score(y_test, y_pred_binary)))
# print('F1 Score: {:.2f}'.format(f1_score(y_test, y_pred_binary)))

# get the roc_auc_score
print(roc_auc_score(y_test, y_pred))

# # plot roc curve
# # get the roc curve
# fpr11, tpr11, thresholds11 = roc_curve(y_test, y_pred)
# # Plot ROC curve
# plt.figure(figsize = (12,8))
# roc11 = roc_auc_score(y_test, y_pred)
# plt.plot(fpr11, tpr11, label='ROC curve (area = %0.3f)' % roc11)
# plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve
# plt.xlim([0.0, 1.0])
# plt.ylim([0.0, 1.0])
# plt.xlabel('False Positive Rate or (1 - Specifity)')
# plt.ylabel('True Positive Rate or (Sensitivity)')
# plt.title('Receiver Operating Characteristic')
# plt.legend(loc="lower right")

plt.figure(figsize = (12,8))
roc10 = roc_auc_score(y_test, y_pred)
plt.plot(fpr1, tpr1, label='ROC curve Logistic Regression (area = %0.3f)' % roc1)
plt.plot(fpr2, tpr2, label='ROC curve KNN (area = %0.3f)' % roc2)
plt.plot(fpr3, tpr3, label='ROC curve SVM (area = %0.3f)' % roc3)
plt.plot(fpr4, tpr4, label='ROC curve Decision Tree (area = %0.3f)' % roc4)
plt.plot(fpr5, tpr5, label='ROC curve Random Forest (area = %0.3f)' % roc5)
plt.plot(fpr6, tpr6, label='ROC curve AdaBoost (area = %0.3f)' % roc6)
plt.plot(fpr7, tpr7, label='ROC curve Gradient Boost (area = %0.3f)' % roc7)
plt.plot(fpr8, tpr8, label='ROC curve Artificial Neural Network model - Single Hidden Layer(area = %0.3f)' % roc8)
plt.plot(fpr9, tpr9, label='ROC curve Artificial Neural Network model - Multi-Layer Preceptron(area = %0.3f)' % roc9)
plt.plot(fpr10, tpr10, label='ROC curve TensorFlow Neural Network Model with 5 Layers (area = %0.3f)' % roc10)
# plt.plot(fpr11, tpr11, label='ROC curve Isolation Forest (area = %0.3f)' % roc11)
plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate or (1 - Specifity)')
plt.ylabel('True Positive Rate or (Sensitivity)')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")

plt.savefig("roc-curves.png")

